{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7UZrOIG5I5h"
   },
   "outputs": [],
   "source": [
    "# !pip3 install -U google-cloud-aiplatform\n",
    "# import IPython\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mtsj7WZcCCUM"
   },
   "source": [
    "# Inner training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LExCT9Dr5KT9"
   },
   "outputs": [],
   "source": [
    "%%writefile training_script.py\n",
    "\n",
    "# Source: https://cloud.google.com/ai-platform-unified/docs/tutorials/image-recognition-custom\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import hypertune\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "\n",
    "def normalize_img(image):\n",
    "    \"\"\"Normalizes image.\n",
    "\n",
    "    * Resizes image to IMG_WIDTH x IMG_WIDTH pixels\n",
    "    * Casts values from `uint8` to `float32`\n",
    "    * Scales values from [0, 255] to [0, 1]\n",
    "\n",
    "    Returns:\n",
    "      A tensor with shape (IMG_WIDTH, IMG_WIDTH, 3). (3 color channels)\n",
    "    \"\"\"\n",
    "    image = tf.image.resize_with_pad(image, IMG_WIDTH, IMG_WIDTH)\n",
    "    return image / 255.\n",
    "\n",
    "\n",
    "def normalize_img_and_label(image, label):\n",
    "    \"\"\"Normalizes image and label.\n",
    "\n",
    "    * Performs normalize_img on image\n",
    "    * Passes through label unchanged\n",
    "\n",
    "    Returns:\n",
    "      Tuple (image, label) where\n",
    "      * image is a tensor with shape (IMG_WIDTH, IMG_WIDTH, 3). (3 color\n",
    "        channels)\n",
    "      * label is an unchanged integer [0, 4] representing flower type\n",
    "    \"\"\"\n",
    "    return normalize_img(image), label\n",
    "\n",
    "def get_args():\n",
    "  \"\"\"Argument parser.\n",
    "  Returns:\n",
    "    Dictionary of arguments.\n",
    "  \"\"\"\n",
    "  parser = argparse.ArgumentParser(description='Flower classification sample')\n",
    "  parser.add_argument(\n",
    "      '--tfds',\n",
    "      default=None,\n",
    "      help='The tfds URI from https://www.tensorflow.org/datasets/ to load the data from')\n",
    "\n",
    "  parser.add_argument('--lr', type=float, default=0.01)\n",
    "  parser.add_argument('--units', type=int, default=4)\n",
    "  parser.add_argument('--activation', type=str, default='relu')\n",
    "  parser.add_argument('--batch_size', type=int, default=128)\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._hp_tune_reporter = hypertune.HyperTune()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self._hp_tune_reporter.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='loss',\n",
    "            metric_value=logs['loss'],\n",
    "            global_step=epoch)\n",
    "\n",
    "# Training settings\n",
    "args = get_args()\n",
    "\n",
    "if 'AIP_MODEL_DIR' not in os.environ:\n",
    "    raise KeyError(\n",
    "        'The `AIP_MODEL_DIR` environment variable has not been' +\n",
    "        'set. See https://cloud.google.com/ai-platform-unified/docs/tutorials/image-recognition-custom/training'\n",
    "    )\n",
    "output_directory = os.environ['AIP_MODEL_DIR']\n",
    "\n",
    "logging.info('Loading and preprocessing data ...')\n",
    "dataset = tfds.load(args.tfds,\n",
    "                    split='train',\n",
    "                    try_gcs=True,\n",
    "                    shuffle_files=True,\n",
    "                    as_supervised=True)\n",
    "dataset = dataset.map(normalize_img_and_label,\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(args.batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "logging.info('Creating and training model ...')\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(args.units,\n",
    "                           3,\n",
    "                           padding='same',\n",
    "                           activation='relu',\n",
    "                           input_shape=(IMG_WIDTH, IMG_WIDTH, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(args.units * 2, 3, padding='same', activation=args.activation),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(args.units * 3, 3, padding='same', activation=args.activation),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=args.activation),\n",
    "    tf.keras.layers.Dense(5)  # 5 classes\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=args.lr),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "model.fit(dataset, epochs=10, callbacks=[HPTCallback()])\n",
    "\n",
    "logging.info(f'Exporting SavedModel to: {output_directory}')\n",
    "# Add softmax layer for intepretability\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "probability_model.save(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBfs1KvC5tpq"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NrQzsYd8mP1"
   },
   "source": [
    "# Construct the Custom Training Job To pass into HP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn3iUSP95lkQ"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "aiplatform.init(project='sashaproject-1',\n",
    "                staging_bucket='gs://ucaip-mb-sasha-dev')\n",
    "\n",
    "job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name='my-job',\n",
    "    script_path='training_script.py',\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-2:latest\",\n",
    "    requirements=[\"gcsfs==0.7.1\", \"cloudml-hypertune\"],\n",
    "    args=[\"--tfds\", \"tf_flowers:3.*.*\"],\n",
    "    replica_count=1,\n",
    "    accelerator_count=1,\n",
    "    accelerator_type='NVIDIA_TESLA_K80',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAALAFfx8zfC"
   },
   "source": [
    "# Run HP Tuning Jub with constructed Custom Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_n0i8315h7e"
   },
   "outputs": [],
   "source": [
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name='hp-test',\n",
    "    custom_job=job,\n",
    "    metric_spec={'loss': 'minimize'},\n",
    "    parameter_spec={\n",
    "        'lr': hpt.DoubleParameterSpec(min=0.001, max=0.1, scale='log'),\n",
    "        'units': hpt.IntegerParameterSpec(min=4, max=32, scale='linear'),\n",
    "        'activation': hpt.CategoricalParameterSpec(values=['relu', 'sigmoid', 'elu', 'selu', 'tanh']),\n",
    "        'batch_size': hpt.DiscreteParameterSpec(values=[32, 64, 128], scale='linear')\n",
    "    },\n",
    "    max_trial_count=128,\n",
    "    parallel_trial_count=8,    \n",
    "    )\n",
    "\n",
    "hp_job.run()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM_CvGu988iP"
   },
   "source": [
    "# You can run the Custom Job stand alone as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zc9wdqaY7KAf"
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VX7CuGKBHvM"
   },
   "source": [
    "Alternatively, you can create a Custom Job from worker pools specs. We'll reuse the Python Package we created fromt the local script above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML8cMQK1BHWE"
   },
   "outputs": [],
   "source": [
    "worker_pool_specs = [\n",
    "   {\n",
    "       \"replica_count\": 1,\n",
    "       \"machine_spec\": {\n",
    "         \"machine_type\": 'n1-standard-4',\n",
    "       },\n",
    "       \"python_package_spec\": {\n",
    "           \"executor_image_uri\": 'us-docker.pkg.dev/cloud-aiplatform/training/tf-cpu.2-4:latest',\n",
    "           \"package_uris\": job.job_spec.worker_pool_specs[0].python_package_spec.package_uris,\n",
    "           \"python_module\": \"aiplatform_custom_trainer_script.task\",\n",
    "           \"args\": [\"--tfds\", \"tf_flowers:3.*.*\"]\n",
    "       },\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2LI1LDYAb1x"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomJob(display_name='test-from_worker_pool_spec',\n",
    "                           worker_pool_specs=worker_pool_specs)\n",
    "job.run()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MB SDK: Custom Job and HP Tuning Fish Food notebook ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
